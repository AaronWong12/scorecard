% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/perf.R
\name{perf_plot}
\alias{perf_plot}
\title{ks value and ks curve}
\usage{
perf_plot(label, pred, groupnum = 20, type = c("ks", "roc"), plot = TRUE,
  seed = 186)
}
\arguments{
\item{label}{label values, such as 0s and 1s.}

\item{pred}{predicted probability values.}

\item{groupnum}{the number of group numbers, default: 20.}

\item{type}{performance plot types: "ks","lift","roc","pr". Default: c("ks").}

\item{plot}{logical value, default: TRUE.}

\item{seed}{seed value for random sort data frame, defalut: 186.}
}
\value{
ks value and ks curve.
}
\description{
This function calculate kolmogorov-smirnow(ks) value and plot ks curve based on provided label and predicted probability values.
}
\examples{
# library(woebin)
# # load germancredit data
# data("germancredit")
# xy <- names(germancredit)
# # data.table
# set.seed(1255)
# dt <- setnames(
#   data.table(germancredit)[sample(nrow(germancredit))],
#   c(paste0("x",1:20), "y")
# )[, y:=ifelse(y=="bad", 1, 0)]
# # x y names
# y <- "y"
# xs <- setdiff(names(dt), y)
#
# # iv woe filter ------
# # variable filter I
# dt_filter <- var_filter(dt, y)
#
# # woe binning
# bins <- woebin(dt_filter, y, stop_limit = 0.05)$bins
# dt_woe <- woebin_ply(dt_filter, bins, y)
#
# # variable filter II
# dt_woe_filter <- var_filter(dt_woe, y)
#
# # # lasso filter ------
# # library(h2o)
# # # h2o data
# # localH2O <- h2o.init()
# # dth2o <- as.h2o(dt_woe_filter)
# #
# # # Breaking Data into Training and Test Sample
# # set.seed(345)
# # dt.split <- h2o.splitFrame(data=dth2o, ratios=0.7)
# # dt.train <- dt.split[[1]]; dt.test <- dt.split[[2]];
# #
# # # h2o.glm lasso ------
# # fit <- h2o.glm(x=names(dt_woe_filter), y, dt.train, validation_frame=dt.test, family = "binomial", nfolds = 0, alpha = 1, lambda_search = TRUE)
# # # summary(fit)
# # h2o_var <- data.table(h2o.varimp(fit))[!is.na(coefficients) & coefficients > 0]
# # dt_woe_lasso <- dt_woe_filter[, c(h2o_var$names, "flagy"), with=FALSE]
#
# # Breaking Data into Training and Test Sample
# set.seed(345)
# d <- sample(nrow(dt_woe_filter), nrow(dt_woe_filter)*0.6)
# train <- dt_woe_filter[d]; test <- dt_woe_filter[-d];
#
# # Traditional Credit Scoring Using Logistic Regression ######
# # a. model I ------
# # remove variables that coefficients == NEG or Pr_z > 0.1
# rm_var_num <- 1
# sel_var <- names(train) #names(xy_selected_lasso)
# while (rm_var_num > 0) {
#   print(rm_var_num)
#
#   m1 <- glm(
#     y ~ ., family = "binomial",
#     data = train[, sel_var, with=FALSE]
#   )
#
#   # coefficients
#   m1_coef <- data.frame(summary(m1)$coefficients)
#   m1_coef$var <- row.names(m1_coef)
#   m1_coef <- data.table(m1_coef)[var != "(Intercept)"]
#   setnames(m1_coef, c("Estimate", "Std_Error", "z_value", "Pr_z", "var"))
#
#   # selected variables
#   sel_var <- c(m1_coef[Estimate > 0 & Pr_z < 0.1, var], y)
#
#   # number of variables that coefficients == NEG or Pr_z > 0.1
#   rm_var_num <- m1_coef[Estimate <= 0 | Pr_z > 0.1][, .N]
# }
# # summary(m1)
#
# # b. model II
# # Select a formula-based model by AIC
# m_step <- step(m1, direction="both")
# m2 <- eval(m_step$call)
# # summary(m2)
#
# # score performance ------
# # score predict
# train$score <- predict(m2, type='response', train)
# test$score <- predict(m2, type='response', test)
#
# # performace plot
# perf_plot(train$y, train$score)
# perf_plot(test$y, test$score)

}
